# ğŸ“Œ í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸°
import pandas as pd  # ë°ì´í„°í”„ë ˆì„ í˜•íƒœë¡œ ë°ì´í„°ë¥¼ ë‹¤ë£¨ê¸° ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
import joblib        # í•™ìŠµëœ ëª¨ë¸ì´ë‚˜ ë°ì´í„°ë¥¼ íŒŒì¼ë¡œ ì €ì¥í•˜ê³  ë¶ˆëŸ¬ì˜¤ê¸° ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
import numpy as np   # ìˆ«ì ê³„ì‚° ë° í†µê³„ ì²˜ë¦¬ìš© ë¼ì´ë¸ŒëŸ¬ë¦¬

# í‰ê°€ ë° ëª¨ë¸ ê´€ë ¨ í•¨ìˆ˜ ë¶ˆëŸ¬ì˜¤ê¸°
from sklearn.metrics import accuracy_score               # ì˜ˆì¸¡ ê²°ê³¼ì˜ ì •í™•ë„ë¥¼ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜
from sklearn.linear_model import LogisticRegression      # ë¡œì§€ìŠ¤í‹± íšŒê·€ ëª¨ë¸ (ì„ í˜• ë¶„ë¥˜)
from sklearn.ensemble import RandomForestClassifier      # ëœë¤í¬ë ˆìŠ¤íŠ¸ (íŠ¸ë¦¬ ê¸°ë°˜ ì•™ìƒë¸” ëª¨ë¸)
from sklearn.neighbors import KNeighborsClassifier       # K-ìµœê·¼ì ‘ ì´ì›ƒ(KNN) ë¶„ë¥˜ê¸°
from sklearn.svm import SVC                              # ì„œí¬íŠ¸ ë²¡í„° ë¨¸ì‹ (SVM)
from xgboost import XGBClassifier                        # XGBoost ëª¨ë¸ (íŠ¸ë¦¬ ê¸°ë°˜ ë¶€ìŠ¤íŒ…)
from sklearn.model_selection import cross_val_score      # êµì°¨ê²€ì¦ ìˆ˜í–‰ í•¨ìˆ˜

# ------------------------------------------------------------------------------

# 1ï¸âƒ£ í•™ìŠµ/ê²€ì¦ ë°ì´í„° ë¡œë“œ (5ì¼ì°¨ì—ì„œ ì €ì¥í•œ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°)
# split_data.pklì—ëŠ” X_train, X_val, y_train, y_valê°€ ì €ì¥ë˜ì–´ ìˆìŒ
X_train, X_val, y_train, y_val = joblib.load('./split_data.pkl')

# ------------------------------------------------------------------------------

# 2ï¸âƒ£ ì „ì²´ train ë°ì´í„°ë¥¼ êµì°¨ê²€ì¦ìš©ìœ¼ë¡œ ë‹¤ì‹œ ë¶ˆëŸ¬ì˜´
train = pd.read_csv('C:/csv/train.csv')  # ì›ë³¸ train.csv ë¶ˆëŸ¬ì˜¤ê¸°

# ì‚¬ìš©í•  ì…ë ¥ í”¼ì²˜ ì •ì˜ (ê²°ì¸¡ì¹˜ ìˆëŠ” í–‰ì€ ì œê±°)
features = ['Pclass', 'Sex', 'Age', 'Fare', 'Embarked']
train = train.dropna(subset=features)

# ë¬¸ìì—´ ë°ì´í„°ë¥¼ ìˆ«ìë¡œ ë³€í™˜ (One-Hot Encoding)
X = pd.get_dummies(train[features], drop_first=True)

# ì •ë‹µ(label) ë°ì´í„° ì •ì˜
y = train['Survived']

# ------------------------------------------------------------------------------

# 3ï¸âƒ£ ì‚¬ìš©í•  ë¨¸ì‹ ëŸ¬ë‹ ë¶„ë¥˜ ëª¨ë¸ë“¤ì„ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ì •ì˜
models = {
    "Logistic Regression": LogisticRegression(max_iter=1000),  # ì„ í˜• ëª¨ë¸ (í™•ë¥  ê¸°ë°˜)
    "Random Forest": RandomForestClassifier(),                 # íŠ¸ë¦¬ ì•™ìƒë¸” ëª¨ë¸
    "KNN": KNeighborsClassifier(),                             # ê°€ì¥ ê°€ê¹Œìš´ ì´ì›ƒ ê¸°ë°˜ ëª¨ë¸
    "SVM": SVC(),                                              # ë²¡í„° ê¸°ë°˜ ê²½ê³„ ë¶„ë¥˜ ëª¨ë¸
    "XGBoost": XGBClassifier(use_label_encoder=False, eval_metric="logloss")  # ë¶€ìŠ¤íŒ… ê¸°ë°˜ ê³ ì„±ëŠ¥ ëª¨ë¸
}

# ------------------------------------------------------------------------------

# 4ï¸âƒ£ ê²€ì¦ìš© ë°ì´í„°ì…‹(X_val)ì— ëŒ€í•´ ê° ëª¨ë¸ì˜ ì •í™•ë„ ì¶œë ¥
print("ğŸ“Œ ê²€ì¦ì…‹ ê¸°ë°˜ ì •í™•ë„ (Validation Accuracy)")
for name, model in models.items():
    model.fit(X_train, y_train)              # ëª¨ë¸ í•™ìŠµ
    preds = model.predict(X_val)             # ê²€ì¦ì…‹ ì˜ˆì¸¡
    acc = accuracy_score(y_val, preds)       # ì •í™•ë„ ê³„ì‚°
    print(f"{name} Accuracy: {acc:.4f}")     # ê²°ê³¼ ì¶œë ¥

# ------------------------------------------------------------------------------

# 5ï¸âƒ£ êµì°¨ê²€ì¦(Cross Validation)ì„ í†µí•´ í‰ê·  ì •í™•ë„ ë° ëª¨ë¸ ì•ˆì •ì„± í‰ê°€
print("\nğŸ“Œ êµì°¨ê²€ì¦ (5-Fold) ê¸°ë°˜ í‰ê·  ì •í™•ë„")
for name, model in models.items():
    scores = cross_val_score(model, X, y, cv=5)  # ë°ì´í„°ë¥¼ 5ê°œë¡œ ë‚˜ëˆ  í‰ê·  ì •í™•ë„ ì¸¡ì •
    print(f"{name} í‰ê·  ì •í™•ë„: {np.mean(scores):.4f}, í‘œì¤€í¸ì°¨: {np.std(scores):.4f}")
