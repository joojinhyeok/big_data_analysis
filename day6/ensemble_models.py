# ğŸ“Œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸°
import pandas as pd  # ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° ë° ì „ì²˜ë¦¬ìš©
import joblib        # í”¼ì²˜ ìˆœì„œ ì €ì¥ ë“± íŒŒì¼ ì €ì¥/ë¶ˆëŸ¬ì˜¤ê¸°ìš©
from sklearn.model_selection import train_test_split  # í•™ìŠµ/ê²€ì¦ ë°ì´í„° ë¶„í• 
from sklearn.metrics import accuracy_score            # ì˜ˆì¸¡ ê²°ê³¼ í‰ê°€ (ì •í™•ë„ ê³„ì‚°)

# ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ë“¤ (ë‹¨ì¼ ë° ì•™ìƒë¸”)
from sklearn.ensemble import VotingClassifier, StackingClassifier, RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from xgboost import XGBClassifier  # ê³ ì„±ëŠ¥ íŠ¸ë¦¬ ê¸°ë°˜ ë¶€ìŠ¤íŒ… ëª¨ë¸

# ------------------------------------------------------------------------------

# 1ï¸âƒ£ í•™ìŠµìš© ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
train = pd.read_csv('C:/csv/train.csv')  # train.csv íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°

# ì‚¬ìš©í•  í”¼ì²˜ ì •ì˜ ë° ê²°ì¸¡ì¹˜ ì œê±°
features = ['Pclass', 'Sex', 'Age', 'Fare', 'Embarked']
train = train.dropna(subset=features)

# ë²”ì£¼í˜• ë°ì´í„°(ë¬¸ìì—´)ë¥¼ ìˆ«ìë¡œ ë³€í™˜ (One-Hot Encoding)
X = pd.get_dummies(train[features], drop_first=True)

# ìƒì¡´ ì—¬ë¶€(label) ì •ì˜
y = train['Survived']

# í•™ìŠµìš©(X_train)ê³¼ ê²€ì¦ìš©(X_val) ë°ì´í„° ë¶„ë¦¬
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

# ------------------------------------------------------------------------------

# 2ï¸âƒ£ ê°œë³„ ëª¨ë¸ ì •ì˜
lr = LogisticRegression(max_iter=1000)         # ë¡œì§€ìŠ¤í‹± íšŒê·€: ì„ í˜• ë¶„ë¥˜ ëª¨ë¸
rf = RandomForestClassifier()                  # ëœë¤ í¬ë ˆìŠ¤íŠ¸: ì—¬ëŸ¬ íŠ¸ë¦¬ë¥¼ í‰ê· ë‚´ëŠ” ì•™ìƒë¸”
knn = KNeighborsClassifier()                   # KNN: ê°€ì¥ ê°€ê¹Œìš´ ë°ì´í„°ë“¤ì˜ í´ë˜ìŠ¤ë¥¼ ë”°ë¦„
xgb = XGBClassifier(eval_metric='logloss')     # XGBoost: ì„±ëŠ¥ ì¢‹ì€ ë¶€ìŠ¤íŒ… ëª¨ë¸ (íŠ¸ë¦¬ ê¸°ë°˜)

# ------------------------------------------------------------------------------

# 3ï¸âƒ£ íˆ¬í‘œ ê¸°ë°˜ ì•™ìƒë¸” ëª¨ë¸ ì •ì˜ (VotingClassifier)
# â–¶ ì—¬ëŸ¬ ëª¨ë¸ì˜ ì˜ˆì¸¡ì„ 'íˆ¬í‘œ'ë¡œ ê²°ì •í•¨

# Hard Voting: ê° ëª¨ë¸ì˜ ìµœì¢… ì˜ˆì¸¡ ê²°ê³¼(í´ë˜ìŠ¤)ë¥¼ ë‹¤ìˆ˜ê²°ë¡œ ì„ íƒ
voting_hard = VotingClassifier(estimators=[
    ('lr', lr), ('rf', rf), ('knn', knn)
], voting='hard')

# Soft Voting: ê° ëª¨ë¸ì˜ í™•ë¥  ì˜ˆì¸¡ ê²°ê³¼ë¥¼ í‰ê· ë‚´ì–´ ê²°ì •
voting_soft = VotingClassifier(estimators=[
    ('lr', lr), ('rf', rf), ('xgb', xgb)
], voting='soft')

# ------------------------------------------------------------------------------

# 4ï¸âƒ£ ìŠ¤íƒœí‚¹ ê¸°ë°˜ ì•™ìƒë¸” ëª¨ë¸ ì •ì˜ (StackingClassifier)
# â–¶ ì—¬ëŸ¬ ëª¨ë¸ì˜ ì˜ˆì¸¡ì„ í•˜ë‚˜ì˜ 'ìµœì¢… ëª¨ë¸'ì´ ë‹¤ì‹œ í•™ìŠµí•˜ì—¬ ê²°í•©í•¨

stacking = StackingClassifier(
    estimators=[('rf', rf), ('xgb', xgb)],              # 1ì¸µ(base) ëª¨ë¸
    final_estimator=LogisticRegression()               # 2ì¸µ(meta) ëª¨ë¸
)

# ------------------------------------------------------------------------------

# 5ï¸âƒ£ ëª¨ë¸ í‰ê°€ í•¨ìˆ˜ ì •ì˜
def evaluate_model(name, model):
    model.fit(X_train, y_train)                         # ëª¨ë¸ í•™ìŠµ
    preds = model.predict(X_val)                        # ê²€ì¦ ë°ì´í„°ë¡œ ì˜ˆì¸¡ ìˆ˜í–‰
    acc = accuracy_score(y_val, preds)                  # ì˜ˆì¸¡ ì •í™•ë„ ê³„ì‚°
    print(f"{name} Accuracy: {acc:.4f}")                # ê²°ê³¼ ì¶œë ¥

# ------------------------------------------------------------------------------

# 6ï¸âƒ£ ê° ì•™ìƒë¸” ëª¨ë¸ ì„±ëŠ¥ í‰ê°€
# -> Votingê³¼ Stacking ëª¨ë¸ë“¤ì´ ê²€ì¦ ë°ì´í„°ì…‹ì—ì„œ ì–¼ë§ˆë‚˜ ì •í™•í•œ ì˜ˆì¸¡ì„ í•˜ëŠ”ì§€ í™•ì¸
evaluate_model("Voting (Hard)", voting_hard)
evaluate_model("Voting (Soft)", voting_soft)
evaluate_model("Stacking", stacking)
