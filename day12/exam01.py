# --------------------------------------------------------------------
# 1. StackingClassifier ì‹¤ìŠµ
# --------------------------------------------------------------------

# 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸°
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import StackingClassifier
from sklearn.metrics import accuracy_score

# 2. ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
df = pd.read_csv('C:/csv/train.csv')  # Titanic ë°ì´í„° ì˜ˆì‹œ

# 3. ì „ì²˜ë¦¬ (ê°„ë‹¨ ë²„ì „)
df = df[['Pclass', 'Sex', 'Age', 'Survived']].dropna()
df['Sex'] = LabelEncoder().fit_transform(df['Sex'])  # male=1, female=0

X = df.drop('Survived', axis=1)
y = df['Survived']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 4. ëª¨ë¸ êµ¬ì„±
estimators = [
    ('dt', DecisionTreeClassifier(random_state=42)),
    ('knn', KNeighborsClassifier())
]

stack_model = StackingClassifier(
    estimators=estimators,
    final_estimator=LogisticRegression()
)

# 5. í•™ìŠµ ë° ì˜ˆì¸¡
stack_model.fit(X_train, y_train)
pred = stack_model.predict(X_test)

# 6. í‰ê°€
print("StackingClassifier Accuracy:", accuracy_score(y_test, pred))

# --------------------------------------------------------------------
# 2. Stacking ëª¨ë¸ ì„±ëŠ¥ í–¥ìƒ - GridSearchCV ì ìš©
# --------------------------------------------------------------------
# 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸°
from sklearn.model_selection import GridSearchCV

# 2. ê¸°ë°˜ ëª¨ë¸ ì¤‘ í•˜ë‚˜ì¸ DecisionTreeì— ëŒ€í•´ GridSearchCV ì ìš©
param_grid = {
    'max_depth': [3, 5, 7],
    'min_samples_split': [2, 4, 6]
}

# 3. ê¸°ë³¸ ëª¨ë¸ ì •ì˜
dt = DecisionTreeClassifier(random_state=42)

# 4. ê·¸ë¦¬ë“œ ì„œì¹˜ ì‹¤í–‰
grid_dt = GridSearchCV(dt, param_grid, cv=5, n_jobs=1)
grid_dt.fit(X_train, y_train)

print("Best Params (DecisionTree): ", grid_dt.best_params_)

# 5. íŠœë‹ëœ ëª¨ë¸ì„ ê¸°ë°˜ ëª¨ë¸ë¡œ ì‚¬ìš©í•˜ì—¬ ìŠ¤íƒœí‚¹ êµ¬ì„±
estimators = [
    ('dt', grid_dt.best_estimator_), # íŠœë‹ëœ ê²°ì •íŠ¸ë¦¬ ì‚¬ìš©
    ('knn', KNeighborsClassifier())
]

stack_model = StackingClassifier(
    estimators=estimators,
    final_estimator=LogisticRegression()
)

# 6. í•™ìŠµ ë° í‰ê°€
stack_model.fit(X_train, y_train)
pred = stack_model.predict(X_test)
print("Tuned StackingClassifier Accuracy: ", accuracy_score(y_test, pred))

# --------------------------------------------------------------------
# 3. êµì°¨ê²€ì¦ìœ¼ë¡œ í‰ê°€í•´ë³´ê¸°
# - ì •í™•ë„ ë¹„êµ ì‹œ ë‹¨ì¼ í…ŒìŠ¤íŠ¸ì…‹ ë§ê³  K-Fold êµì°¨ê²€ì¦ìœ¼ë¡œ "í‰ê·  ì •í™•ë„" í™•ì¸
#   í•˜ëŠ”ê²Œ ë” ì•ˆì •ì ì¸ í‰ê°€
# --------------------------------------------------------------------
# 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ê°€ì ¸ì˜¤ê¸°
from sklearn.model_selection import cross_val_score

# 2. êµì°¨ê²€ì¦ìœ¼ë¡œ í‰ê·  ì •í™•ë„ í™•ì¸
scores = cross_val_score(stack_model, X, y, cv=5, scoring='accuracy')

print("êµì°¨ê²€ì¦ ì •í™•ë„ ê°(Fold)", scores)
print("í‰ê·  ì •í™•ë„: ", scores.mean())

# --------------------------------------------------------------------
# 4. RandomizedSearchCV ì ìš© ì‹¤ìŠµ
# --------------------------------------------------------------------
# 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸°
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import randint

# 2. íƒìƒ‰ ë²”ìœ„ ì •ì˜ (ë‚œìˆ˜ ë¶„í¬ë¡œ ì •ì˜)
param_dist = {
    'max_depth': randint(3, 10),             # 3~9
    'min_samples_split': randint(2, 10)      # 2~9
}

# 3. ëœë¤ ì„œì¹˜ ì‹¤í–‰
random_dt = RandomizedSearchCV(
    estimator = DecisionTreeClassifier(random_state=42),
    param_distributions=param_dist,
    n_iter=10,              # ëœë¤í•˜ê²Œ 10ì¡°í•© ë½‘ì•„ ì‹œí—˜
    cv=5,
    random_state=42,
    n_jobs=1                # ì‹œí—˜ í™˜ê²½ ê³ ë ¤í•´ 1ë¡œ ê³ ì •
)

random_dt.fit(X_train, y_train)

print("Best Parms (Randomized):", random_dt.best_params_)

# 4. ìµœì  ëª¨ë¸ì„ ê¸°ë°˜ìœ¼ë¡œ ìŠ¤íƒœí‚¹ êµ¬ì„±
estimators = [
    ('dt', random_dt.best_estimator_),  # íŠœë‹ëœ ê²°ì •íŠ¸ë¦¬
    ('knn', KNeighborsClassifier())
]

stack_model = StackingClassifier(
    estimators=estimators,
    final_estimator=LogisticRegression()
)

# 5. í•™ìŠµ ë° ì˜ˆì¸¡
stack_model.fit(X_train, y_train)
pred = stack_model.predict(X_test)

print("Tuned StackingClassifier Accuracy (Randomized):", accuracy_score(y_test, pred))

### âœ” ëª¨ë¸ ì •í™•ë„ ë¹„êµ

# - ê¸°ë³¸ ìŠ¤íƒœí‚¹ ëª¨ë¸ ì •í™•ë„: **0.748**
# - GridSearchCV íŠœë‹ ê²°ê³¼: **0.741**
# - RandomizedSearchCV íŠœë‹ ê²°ê³¼: **0.734**
# - êµì°¨ê²€ì¦ í‰ê·  ì •í™•ë„: **0.798**

# ğŸ§  **ë¶„ì„**: íŠœë‹ì„ í†µí•´ ìµœì ì˜ íŒŒë¼ë¯¸í„°ë¥¼ ì°¾ì•˜ì§€ë§Œ, ì‹¤ì œ í…ŒìŠ¤íŠ¸ì…‹ì—ì„œëŠ” ì •í™•ë„ê°€ ë‹¤ì†Œ ë‚®ê²Œ ë‚˜ì˜¬ ìˆ˜ ìˆë‹¤.  
# ì´ëŠ” êµì°¨ê²€ì¦ê³¼ ì‹¤ì œ ë¶„í• ì˜ ì°¨ì´, ë°ì´í„° ì í•©ë„, íŠœë‹ ë²”ìœ„ ì œí•œ ë“±ì˜ ì´ìœ ë¡œ ë¶„ì„ëœë‹¤.
